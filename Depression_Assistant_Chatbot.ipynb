{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'fear', 'sadness', 'joy']\n",
      " Shape of X is  (3783, 1547)\n",
      " Shape of Y is  (4, 1547)\n",
      " Shape of m is  1547\n",
      " Shape of W1 is  (100, 3783)\n",
      " Shape of W2 is  (4, 100)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.28      0.32       531\n",
      "           1       0.02      0.19      0.04        37\n",
      "           2       0.11      0.33      0.16       127\n",
      "           3       0.49      0.23      0.31       852\n",
      "\n",
      "   micro avg       0.26      0.26      0.26      1547\n",
      "   macro avg       0.24      0.26      0.21      1547\n",
      "weighted avg       0.40      0.26      0.30      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.47      0.51       502\n",
      "           1       0.34      0.38      0.36       311\n",
      "           2       0.36      0.44      0.39       313\n",
      "           3       0.49      0.47      0.48       421\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      1547\n",
      "   macro avg       0.44      0.44      0.44      1547\n",
      "weighted avg       0.46      0.44      0.45      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63       455\n",
      "           1       0.26      0.62      0.37       145\n",
      "           2       0.50      0.50      0.50       381\n",
      "           3       0.71      0.51      0.59       566\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      1547\n",
      "   macro avg       0.53      0.56      0.52      1547\n",
      "weighted avg       0.60      0.54      0.56      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       534\n",
      "           1       0.57      0.61      0.59       324\n",
      "           2       0.51      0.70      0.59       281\n",
      "           3       0.69      0.69      0.69       408\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1547\n",
      "   macro avg       0.64      0.65      0.64      1547\n",
      "weighted avg       0.67      0.65      0.65      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76       390\n",
      "           1       0.48      0.78      0.59       215\n",
      "           2       0.69      0.63      0.66       420\n",
      "           3       0.83      0.65      0.73       522\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1547\n",
      "   macro avg       0.69      0.71      0.68      1547\n",
      "weighted avg       0.72      0.69      0.70      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70       679\n",
      "           1       0.71      0.64      0.68       385\n",
      "           2       0.45      0.89      0.59       193\n",
      "           3       0.63      0.88      0.73       290\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1547\n",
      "   macro avg       0.68      0.74      0.68      1547\n",
      "weighted avg       0.76      0.68      0.69      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.98      0.39       101\n",
      "           1       0.25      0.97      0.40        91\n",
      "           2       0.77      0.50      0.61       586\n",
      "           3       0.95      0.50      0.65       769\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      1547\n",
      "   macro avg       0.55      0.74      0.51      1547\n",
      "weighted avg       0.79      0.56      0.60      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.34      0.51      1210\n",
      "           1       0.57      0.71      0.63       278\n",
      "           2       0.02      1.00      0.04         8\n",
      "           3       0.13      1.00      0.22        51\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      1547\n",
      "   macro avg       0.43      0.76      0.35      1547\n",
      "weighted avg       0.89      0.43      0.52      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      1.00      0.00         1\n",
      "           1       0.44      0.94      0.60       163\n",
      "           2       0.84      0.48      0.61       674\n",
      "           3       0.94      0.53      0.68       709\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      1547\n",
      "   macro avg       0.56      0.74      0.47      1547\n",
      "weighted avg       0.84      0.55      0.64      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       423\n",
      "           1       0.83      0.86      0.84       333\n",
      "           2       0.78      0.83      0.81       361\n",
      "           3       0.89      0.84      0.86       430\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1547\n",
      "   macro avg       0.84      0.85      0.84      1547\n",
      "weighted avg       0.85      0.85      0.85      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       417\n",
      "           1       0.82      0.89      0.85       318\n",
      "           2       0.78      0.86      0.82       348\n",
      "           3       0.93      0.81      0.87       464\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1547\n",
      "   macro avg       0.85      0.86      0.85      1547\n",
      "weighted avg       0.86      0.86      0.86      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       422\n",
      "           1       0.87      0.86      0.87       351\n",
      "           2       0.79      0.88      0.83       347\n",
      "           3       0.92      0.87      0.90       427\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1547\n",
      "   macro avg       0.87      0.87      0.87      1547\n",
      "weighted avg       0.88      0.87      0.87      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88       453\n",
      "           1       0.84      0.92      0.88       319\n",
      "           2       0.79      0.89      0.84       344\n",
      "           3       0.93      0.87      0.90       431\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1547\n",
      "   macro avg       0.87      0.88      0.88      1547\n",
      "weighted avg       0.88      0.88      0.88      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       412\n",
      "           1       0.93      0.85      0.89       382\n",
      "           2       0.83      0.91      0.86       349\n",
      "           3       0.91      0.91      0.91       404\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1547\n",
      "   macro avg       0.89      0.89      0.89      1547\n",
      "weighted avg       0.90      0.89      0.89      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       484\n",
      "           1       0.78      0.97      0.87       279\n",
      "           2       0.85      0.88      0.86       370\n",
      "           3       0.93      0.91      0.92       414\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1547\n",
      "   macro avg       0.88      0.89      0.88      1547\n",
      "weighted avg       0.89      0.88      0.88      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88       351\n",
      "           1       0.98      0.66      0.79       515\n",
      "           2       0.74      0.93      0.82       304\n",
      "           3       0.87      0.93      0.90       377\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1547\n",
      "   macro avg       0.85      0.87      0.85      1547\n",
      "weighted avg       0.87      0.85      0.84      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80       593\n",
      "           1       0.25      1.00      0.40        88\n",
      "           2       0.88      0.73      0.80       462\n",
      "           3       0.91      0.91      0.91       404\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1547\n",
      "   macro avg       0.76      0.83      0.73      1547\n",
      "weighted avg       0.89      0.77      0.81      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.99      0.59       173\n",
      "           1       1.00      0.38      0.55       907\n",
      "           2       0.41      0.97      0.58       163\n",
      "           3       0.71      0.95      0.82       304\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1547\n",
      "   macro avg       0.64      0.82      0.63      1547\n",
      "weighted avg       0.81      0.62      0.61      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.68      0.79       578\n",
      "           1       0.02      1.00      0.05         8\n",
      "           2       0.93      0.64      0.76       565\n",
      "           3       0.90      0.92      0.91       396\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1547\n",
      "   macro avg       0.70      0.81      0.63      1547\n",
      "weighted avg       0.93      0.73      0.81      1547\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90       461\n",
      "           1       0.97      0.74      0.84       453\n",
      "           2       0.60      0.99      0.74       232\n",
      "           3       0.91      0.92      0.92       401\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1547\n",
      "   macro avg       0.86      0.87      0.85      1547\n",
      "weighted avg       0.89      0.86      0.86      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92       368\n",
      "           1       0.71      0.99      0.82       248\n",
      "           2       0.98      0.73      0.83       517\n",
      "           3       0.94      0.92      0.93       414\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1547\n",
      "   macro avg       0.87      0.90      0.88      1547\n",
      "weighted avg       0.90      0.88      0.88      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78       629\n",
      "           1       0.95      0.78      0.86       420\n",
      "           2       0.46      0.99      0.62       176\n",
      "           3       0.78      0.98      0.87       322\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1547\n",
      "   macro avg       0.79      0.85      0.78      1547\n",
      "weighted avg       0.87      0.79      0.80      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      1.00      0.56       161\n",
      "           1       0.59      0.99      0.74       207\n",
      "           2       0.96      0.58      0.73       631\n",
      "           3       0.97      0.72      0.82       548\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1547\n",
      "   macro avg       0.73      0.82      0.71      1547\n",
      "weighted avg       0.85      0.73      0.75      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.46      0.63       884\n",
      "           1       0.88      0.81      0.85       378\n",
      "           2       0.29      0.99      0.45       112\n",
      "           3       0.43      1.00      0.60       173\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1547\n",
      "   macro avg       0.65      0.82      0.63      1547\n",
      "weighted avg       0.86      0.65      0.67      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.39       101\n",
      "           1       0.88      0.94      0.91       326\n",
      "           2       0.95      0.62      0.75       591\n",
      "           3       0.98      0.75      0.85       529\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1547\n",
      "   macro avg       0.77      0.83      0.73      1547\n",
      "weighted avg       0.90      0.76      0.79      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85       555\n",
      "           1       0.90      0.96      0.93       323\n",
      "           2       0.78      0.97      0.87       311\n",
      "           3       0.87      0.99      0.93       358\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1547\n",
      "   macro avg       0.89      0.91      0.89      1547\n",
      "weighted avg       0.90      0.89      0.89      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.93       370\n",
      "           1       0.96      0.90      0.93       368\n",
      "           2       0.94      0.91      0.92       396\n",
      "           3       0.97      0.95      0.96       413\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1547\n",
      "   macro avg       0.94      0.94      0.94      1547\n",
      "weighted avg       0.94      0.94      0.94      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92       479\n",
      "           1       0.88      0.97      0.92       315\n",
      "           2       0.89      0.94      0.92       361\n",
      "           3       0.95      0.98      0.96       392\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1547\n",
      "   macro avg       0.93      0.94      0.93      1547\n",
      "weighted avg       0.94      0.93      0.93      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       382\n",
      "           1       0.97      0.89      0.93       377\n",
      "           2       0.93      0.93      0.93       384\n",
      "           3       0.97      0.97      0.97       404\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1547\n",
      "   macro avg       0.95      0.94      0.94      1547\n",
      "weighted avg       0.95      0.94      0.94      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       471\n",
      "           1       0.88      0.97      0.93       316\n",
      "           2       0.90      0.94      0.92       367\n",
      "           3       0.96      0.98      0.97       393\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1547\n",
      "   macro avg       0.93      0.94      0.94      1547\n",
      "weighted avg       0.94      0.94      0.94      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       390\n",
      "           1       0.97      0.91      0.94       373\n",
      "           2       0.93      0.94      0.93       381\n",
      "           3       0.97      0.97      0.97       403\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       463\n",
      "           1       0.90      0.98      0.94       318\n",
      "           2       0.92      0.94      0.93       374\n",
      "           3       0.96      0.99      0.97       392\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1547\n",
      "   macro avg       0.94      0.95      0.94      1547\n",
      "weighted avg       0.95      0.94      0.94      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       393\n",
      "           1       0.98      0.91      0.94       371\n",
      "           2       0.93      0.94      0.94       379\n",
      "           3       0.97      0.98      0.97       404\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.95      0.95      1547\n",
      "weighted avg       0.96      0.96      0.96      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       455\n",
      "           1       0.91      0.98      0.94       324\n",
      "           2       0.92      0.95      0.94       375\n",
      "           3       0.96      0.99      0.97       393\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       399\n",
      "           1       0.98      0.92      0.95       368\n",
      "           2       0.94      0.95      0.94       378\n",
      "           3       0.98      0.98      0.98       402\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.96      0.96      1547\n",
      "weighted avg       0.96      0.96      0.96      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       439\n",
      "           1       0.94      0.97      0.95       335\n",
      "           2       0.93      0.95      0.94       377\n",
      "           3       0.97      0.99      0.98       396\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.96      0.96      1547\n",
      "weighted avg       0.96      0.96      0.96      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       407\n",
      "           1       0.98      0.94      0.96       360\n",
      "           2       0.95      0.96      0.95       378\n",
      "           3       0.98      0.99      0.98       402\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       435\n",
      "           1       0.95      0.97      0.96       341\n",
      "           2       0.93      0.96      0.94       374\n",
      "           3       0.97      0.99      0.98       397\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.97      0.96      1547\n",
      "weighted avg       0.97      0.96      0.96      1547\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       411\n",
      "           1       0.97      0.96      0.97       351\n",
      "           2       0.96      0.96      0.96       383\n",
      "           3       0.98      0.99      0.98       402\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       430\n",
      "           1       0.97      0.97      0.97       346\n",
      "           2       0.93      0.96      0.95       374\n",
      "           3       0.97      0.99      0.98       397\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       411\n",
      "           1       0.97      0.96      0.97       350\n",
      "           2       0.96      0.96      0.96       384\n",
      "           3       0.98      0.99      0.98       402\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       423\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.94      0.97      0.95       375\n",
      "           3       0.98      0.99      0.99       400\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       412\n",
      "           1       0.97      0.97      0.97       347\n",
      "           2       0.96      0.96      0.96       385\n",
      "           3       0.98      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       421\n",
      "           1       0.97      0.97      0.97       350\n",
      "           2       0.95      0.97      0.96       376\n",
      "           3       0.98      0.99      0.99       400\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       412\n",
      "           1       0.97      0.98      0.97       345\n",
      "           2       0.97      0.96      0.96       388\n",
      "           3       0.98      0.99      0.99       402\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       421\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.95      0.97      0.96       375\n",
      "           3       0.99      0.99      0.99       402\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.97      0.98      0.97      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       412\n",
      "           1       0.97      0.98      0.98       344\n",
      "           2       0.97      0.96      0.96       387\n",
      "           3       0.99      0.99      0.99       404\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       419\n",
      "           1       0.98      0.97      0.97       350\n",
      "           2       0.95      0.97      0.96       376\n",
      "           3       0.99      0.99      0.99       402\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       413\n",
      "           1       0.97      0.98      0.98       344\n",
      "           2       0.97      0.96      0.96       387\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       417\n",
      "           1       0.98      0.97      0.98       350\n",
      "           2       0.96      0.98      0.97       377\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "saved synapses to: weights.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]\n",
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(\"C:/Users/shaya/Desktop/Old laptop data/ML Project/datasets/\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        intensity = float(line.split()[-1])\n",
    "        if (intensity>=threshold):\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-1]\n",
    "            training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset, finds their stem and \n",
    "# encodes each sentence according to the bag of words appending it to training set\n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    labels = []\n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    words=[]\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training set in a numpy array as it is required for calculations in neural net\n",
    "    training_set = np.array(training_set)\n",
    "    \n",
    "    # We convert our labels in numpy array as it is required for calculations in neural net\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(training_set.shape[0])\n",
    "    shuffled_training_set, shuffled_labels = np.zeros((training_set.shape)),np.zeros((training_set.shape))\n",
    "    shuffled_training_set,shuffled_labels= training_set[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(shuffled_training_set,shuffled_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.3\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "    \n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "#    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"classification: %s \\n\" % (return_results))\n",
    "    return return_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a nice day. Goodbye :) \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "input_sentiment = input(\"Hi :) How are you feeling today ? \")\n",
    "sentiment = classify(input_sentiment)\n",
    "list_of_sentiments = []\n",
    "#print(sentiment)\n",
    "for i in range(len(sentiment)):\n",
    "    list_of_sentiments.append(sentiment[i][0])\n",
    "if \"anger\" in list_of_sentiments or \"sadness\" in list_of_sentiments or \"fear\" in list_of_sentiments:\n",
    "    answer = input(\"Sorry to hear that .... would you like to hear a joke to lighten your mood ? Write Yes or No \")    \n",
    "    clear_output()\n",
    "    if answer == \"N\" or answer == \"No\":\n",
    "        print(\"Have a nice day. Goodbye :) \")\n",
    "    else:\n",
    "        # This is where file of jokes.txt is stored in my computer\n",
    "        file = open('C:/Users/Shaya/Desktop/jokes.txt','r')\n",
    "        while(1):\n",
    "            full_file = file.readline()\n",
    "            # The jokes file has a / at the end of every joke which helps distinguish the end of joke.\n",
    "            split_file = full_file.split('/')\n",
    "            slashes = full_file.count('/')\n",
    "            line_of_joke = []\n",
    "            for i in range(slashes):\n",
    "                k=0\n",
    "                commas = split_file[i].count('\"')\n",
    "                length = int(commas/2)\n",
    "                if length == 0:\n",
    "                    line_of_joke.append(split_file[i])\n",
    "                else:\n",
    "                    for j in range(length):        \n",
    "                        # If jokes contain inverted commas then we are taking everything before every 2nd inverted comma in one line and rest\n",
    "                        # comes in next line\n",
    "                        line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
    "                        if j==length-1:\n",
    "                            line_of_joke.append(split_file[i].split('\"')[k+2])\n",
    "                        k=k+2\n",
    "            for i in line_of_joke:\n",
    "                print(i)\n",
    "            user_input = input(\"Do you want another joke ? Write Yes or No\\t\")            \n",
    "            clear_output()\n",
    "            if user_input == \"Y\" or user_input == \"Yes\":\n",
    "                pass\n",
    "            else:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
